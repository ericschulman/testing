{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import vuong_tests10 as vuong_tests_fast\n",
    "from vuong_test_base import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "class JointNormal1(GenericLikelihoodModel):\n",
    "    \n",
    "    def loglikeobs(self, params):\n",
    "        data = np.concatenate([[self.endog],self.exog.transpose()],axis=0)\n",
    "        mult_rv = stats.multivariate_normal([params[0], 0.0], [[1,0],[0,1]])\n",
    "        return mult_rv.logpdf(data.transpose())\n",
    "    \n",
    "    \n",
    "class JointNormal2(GenericLikelihoodModel):\n",
    "    \n",
    "    def loglikeobs(self, params):\n",
    "        data = np.concatenate([[self.endog],self.exog.transpose()],axis=0)\n",
    "        mult_rv = stats.multivariate_normal([0.0, params[0]], [[1,0],[0,1]])\n",
    "        return mult_rv.logpdf(data.transpose())\n",
    "\n",
    "\n",
    "def setup_shi(yn,xn):\n",
    "    # model 1 grad, etc.\n",
    "    nobs = yn.shape[0]\n",
    "    model1_param = np.array([yn.mean()])\n",
    "    model2_param = np.array([xn.mean()])\n",
    "    \n",
    "    model1_deriv = JointNormal1(yn,xn)\n",
    "    ll1 = model1_deriv.loglikeobs(model1_param)\n",
    "    grad1 =  model1_deriv.score_obs(model1_param).reshape( (nobs,1) )\n",
    "    hess1 = model1_deriv.hessian(model1_param)\n",
    "    \n",
    "    \n",
    "    model2_deriv = JointNormal2(yn,xn)\n",
    "    ll2 = model2_deriv.loglikeobs(model2_param)\n",
    "    grad2 =  model2_deriv.score_obs(model2_param).reshape( (nobs,1) )  \n",
    "    hess2 = model2_deriv.hessian(model2_param)\n",
    "    \n",
    "    return ll1,grad1,hess1,model1_param,ll2,grad2,hess2,model2_param\n",
    "\n",
    "def gen_data(beta= 1.5, nobs=1000):\n",
    "    cov = [[25, 0], [0, 1]]\n",
    "    data = np.random.multivariate_normal([beta,beta], [[25,0],[0,1]],  nobs)\n",
    "    return data[:,0],data[:,1],nobs\n",
    "\n",
    "yn,xn,nobs = gen_data()\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "print(grad1.shape,hess1.shape)\n",
    "#NOTE! Weird size distortions with shi's test when theta = .5...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def sw_test_stat(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=.5,compute_v=True,print_stuff=False):\n",
    "    nobs = ll1.shape[0]\n",
    "\n",
    "    idx_even = np.arange(0, nobs, 2)         # 0-based indices: 0,2,4,...\n",
    "    idx_odd  = np.arange(1, nobs, 2)         # 1,3,5,...\n",
    "\n",
    "    # Regular loglikelihood ratio statistic\n",
    "    llr = (ll1 - ll2).sum()\n",
    "    \n",
    "    # Split-sample difference statistic\n",
    "    llr_split = ll1[idx_even].sum() - ll2[idx_odd].sum()\n",
    "\n",
    "    # Regularized numerator (as in your expression)\n",
    "    llr_reg = llr + epsilon * llr_split\n",
    "\n",
    "    # Main variance\n",
    "    omega2 = (ll1 - ll2).var()\n",
    "    \n",
    "    # Split-group variances\n",
    "    omega_A2 = ll1[idx_even].var()\n",
    "    omega_B2 = ll2[idx_odd].var()\n",
    "\n",
    "    # Regularized variance\n",
    "    omega_reg2 = (1 + epsilon) * omega2 + (epsilon**2 / 2) * (omega_A2 + omega_B2)\n",
    "    omega_reg = np.sqrt(omega_reg2)\n",
    "\n",
    "    if print_stuff:\n",
    "        print('llr',llr,llr_split)\n",
    "        print('omega',omega2,omega_A2,omega_B2,np.sqrt(omega2),np.sqrt(omega_reg2))\n",
    "    if not compute_v:\n",
    "        return llr_reg,omega_reg,nobs \n",
    "        \n",
    "    V = compute_eigen2(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2) # If you want to try to bias-correct you can, this is optional\n",
    "    return llr_reg,omega_reg,V,nobs \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def sw_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=.5,\n",
    "            alpha=.05,  biascorrect=False):\n",
    "\n",
    "    llr_reg,omega_reg,V,nobs =  sw_test_stat(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=.5)\n",
    "\n",
    "    if biascorrect:\n",
    "        llr_reg += V.sum() / 2\n",
    "\n",
    "    test_stat = llr_reg / (omega_reg * np.sqrt(nobs))\n",
    "\n",
    "    # Two-sided test\n",
    "    reject_high = (test_stat >= norm.ppf(1 - alpha / 2))\n",
    "    reject_low  = (test_stat <= norm.ppf(alpha / 2))\n",
    "    return int(reject_high) + 2 * int(reject_low)\n",
    "\n",
    "    \n",
    "yn,xn,nobs = gen_data()\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "print(grad1.shape,hess1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_distr(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "                    epsilon=0.5, trials=500, seed=None, biascorrect=False):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    nobs = ll1.shape[0]\n",
    "\n",
    "    # Compute observed statistic\n",
    "    llr_reg, omega_reg, V, _ = sw_test_stat(\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "        epsilon=epsilon, print_stuff=False\n",
    "    )\n",
    "    if biascorrect:\n",
    "        llr_reg += V.sum() / 2\n",
    "    test_stat = llr_reg / (omega_reg * np.sqrt(nobs))\n",
    "    \n",
    "    # Indices for even/odd destinations and source pairs\n",
    "    idx_even = np.arange(0, nobs, 2)\n",
    "    idx_odd  = idx_even + 1\n",
    "    m = idx_even.size\n",
    "    pair_ids = np.arange(m)  # pair j corresponds to source indices (2j, 2j+1)\n",
    "    \n",
    "    stat_dist = []\n",
    "    for b in range(trials):\n",
    "        draw = rng.choice(pair_ids, size=m, replace=True)\n",
    "    \n",
    "        # Source indices for drawn pairs\n",
    "        src_even = idx_even[draw]\n",
    "        src_odd  = idx_odd[draw]\n",
    "    \n",
    "        # Build bootstrap arrays by placing drawn pairs into their parity slots\n",
    "        ll1_s = np.empty_like(ll1)\n",
    "        ll2_s = np.empty_like(ll2)\n",
    "        ll1_s[idx_even] = ll1[src_even]\n",
    "        ll2_s[idx_even] = ll2[src_even]\n",
    "        ll1_s[idx_odd]  = ll1[src_odd]\n",
    "        ll2_s[idx_odd]  = ll2[src_odd]\n",
    "    \n",
    "        # Compute bootstrap statistic (no need to compute V each time)\n",
    "        llr_reg_s, omega_reg_s, nobs_in_s = sw_test_stat(\n",
    "            ll1_s, grad1, hess1, params1, ll2_s, grad2, hess2, params2,\n",
    "            epsilon=epsilon, compute_v=False, print_stuff=False\n",
    "        )\n",
    "        stat_s = (llr_reg_s-llr_reg) / (omega_reg_s * np.sqrt(nobs_in_s))\n",
    "        stat_dist.append(stat_s)\n",
    "    \n",
    "    return np.array(stat_dist) , test_stat\n",
    "\n",
    "def sw_bs_test_helper(stat_dist, stat_obs, alpha=.05, left=True, right=True, print_stuff=False):\n",
    "    cv_upper = np.percentile(stat_dist, 100 * (1 - alpha / 2))\n",
    "    cv_lower = np.percentile(stat_dist, 100 * (alpha / 2))\n",
    "    if print_stuff:\n",
    "        print(f\"mean={stat_dist.mean():.3f}, cv_lower={cv_lower:.3f}, stat_obs={stat_obs:.3f}, cv_upper={cv_upper:.3f}\")\n",
    "    out = 0\n",
    "    if right and (stat_obs > cv_upper):\n",
    "        out = 1\n",
    "    if left and (stat_obs < cv_lower):\n",
    "        out += 2\n",
    "    return out\n",
    "\n",
    "def sw_bs_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "               alpha=.05, trials=500, epsilon=0.5, biascorrect=False, seed=None, print_stuff=False):\n",
    "    stat_dist, test_stat = bootstrap_distr(\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "        epsilon=epsilon, trials=trials, seed=seed, biascorrect=biascorrect\n",
    "    )\n",
    "    return sw_bs_test_helper(stat_dist, test_stat, alpha=alpha, print_stuff=print_stuff)\n",
    "\n",
    "# Example Usage:\n",
    "# result = sw_bs_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=0.5, trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for \"sandwich\" variance components\n",
    "def _estimate_HV(grad, hess, ridge=1e-8):\n",
    "    n = grad.shape[0]\n",
    "    H_hat = -np.mean(hess, axis=0)\n",
    "    H_hat = H_hat + ridge * np.eye(H_hat.shape[0])\n",
    "    V_hat = (grad.T @ grad) / n\n",
    "    return H_hat, V_hat\n",
    "\n",
    "\n",
    "def _trace_HinvV(H, V):\n",
    "    X = np.linalg.solve(H, V)\n",
    "    return float(np.trace(X))\n",
    "\n",
    "\n",
    "def compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, alpha=0.05, ridge=1e-8,\n",
    "                            min_epsilon=1e-6, max_epsilon=10.0):\n",
    "    nobs = ll1.shape[0]\n",
    "    idx_even = np.arange(0, nobs, 2)\n",
    "    idx_odd  = np.arange(1, nobs, 2)\n",
    "\n",
    "    # Variances\n",
    "    sigma2_hat  = float(np.var(ll1 - ll2, ddof=0))\n",
    "    sigmaA2_hat = float(np.var(ll1[idx_even], ddof=0)) if idx_even.size > 1 else 0.0\n",
    "    sigmaB2_hat = float(np.var(ll2[idx_odd],  ddof=0)) if idx_odd.size  > 1 else 0.0\n",
    "    sigma_hat   = np.sqrt(max(sigma2_hat, 1e-12))\n",
    "\n",
    "    # Estimate (H, V) for each model\n",
    "    H1_hat, V1_hat = _estimate_HV(grad1, hess1, ridge=ridge)\n",
    "    H2_hat, V2_hat = _estimate_HV(grad2, hess2, ridge=ridge)\n",
    "    try:\n",
    "        tr1 = abs(_trace_HinvV(H1_hat, V1_hat))\n",
    "        tr2 = abs(_trace_HinvV(H2_hat, V2_hat))\n",
    "        tr_max = max(tr1, tr2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        tr_max = 1.0\n",
    "\n",
    "    # Paper constants\n",
    "    z = norm.ppf(1 - alpha/2.0)\n",
    "    phi_z = norm.pdf(z)\n",
    "    delta_star_hat = (sigma_hat / 2.0) * (z - np.sqrt(4.0 + z**2))\n",
    "    phi_arg = z - (delta_star_hat / max(sigma_hat, 1e-12))\n",
    "    phi_term = norm.pdf(phi_arg)\n",
    "\n",
    "    denom_PL = 4.0 * (sigma_hat**3 + 1e-24)\n",
    "    CPL_num = delta_star_hat * (sigma_hat**2 - 2.0 * (sigmaA2_hat + sigmaB2_hat))\n",
    "    C_PL_hat = phi_term * CPL_num / denom_PL\n",
    "\n",
    "    denom_SD = np.sqrt(max((sigmaA2_hat + sigmaB2_hat)/2.0, 1e-24))\n",
    "    C_SD_hat = 2.0 * phi_z * tr_max / denom_SD\n",
    "\n",
    "    lnln_term = max(np.log(np.log(max(nobs, 3))), 1e-6)\n",
    "    fallback = nobs**(-1/6) * (lnln_term**(1/3))\n",
    "\n",
    "    valid = (np.isfinite(C_PL_hat) and np.isfinite(C_SD_hat) and\n",
    "             C_PL_hat > 0 and C_SD_hat > 0)\n",
    "\n",
    "    if valid:\n",
    "        eps_hat = ((C_SD_hat/C_PL_hat)**(1/3)) * (nobs**(-1/6)) * (lnln_term**(1/3))\n",
    "    else:\n",
    "        eps_hat = fallback\n",
    "\n",
    "    return float(np.clip(eps_hat, min_epsilon, max_epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo2(total, gen_data, setup_shi, alpha=.05,  c1=.5,trials=500, biascorrect=False,epsilon_auto=False):\n",
    "    # Count for [model1, tie, model2] for each test\n",
    "    reg = np.zeros(3, dtype=int)\n",
    "    sw  = np.zeros(3, dtype=int)\n",
    "    swbs = np.zeros(3, dtype=int)\n",
    "\n",
    "    # Tracking statistics\n",
    "    llr_sum = 0.0\n",
    "    llr_sq_sum = 0.0\n",
    "    omega_sum = 0.0\n",
    "    nobs_last = None\n",
    "    \n",
    "    for _ in range(total):\n",
    "        np.random.seed()\n",
    "        yn, xn, nobs = gen_data()\n",
    "\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2 = setup_shi(yn, xn)\n",
    "        \n",
    "        if epsilon_auto:\n",
    "            c1 = compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, ridge=1e-8 , alpha=.05)\n",
    "            \n",
    "        # Update LLR-related stats\n",
    "        llrn = (ll1 - ll2).sum()\n",
    "        omegan = np.sqrt((ll1 - ll2).var())\n",
    "        llr_sum   += llrn\n",
    "        llr_sq_sum += llrn ** 2\n",
    "        omega_sum += omegan\n",
    "        nobs_last = nobs\n",
    "\n",
    "        # Run the tests\n",
    "        reg_idx = regular_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2,\n",
    "            biascorrect=biascorrect, alpha=alpha\n",
    "        )\n",
    "        sw_idx = sw_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2, epsilon = c1,\n",
    "            alpha=alpha, biascorrect=biascorrect\n",
    "        )\n",
    "        swbs_idx = sw_bs_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2, epsilon = c1,\n",
    "            alpha=alpha, biascorrect=biascorrect, print_stuff=False\n",
    "        )\n",
    "\n",
    "        reg[reg_idx]   += 1\n",
    "        sw[sw_idx]     += 1\n",
    "        swbs[swbs_idx] += 1\n",
    "    \n",
    "    # Convert counts to rates/frequencies\n",
    "    reg_freq = reg / total\n",
    "    sw_freq = sw / total\n",
    "    swbs_freq = swbs / total\n",
    "\n",
    "    llr_mean = llr_sum / total\n",
    "    llr_sd   = np.sqrt(llr_sq_sum / total - llr_mean ** 2)\n",
    "    omega_scaled = omega_sum * np.sqrt(nobs_last) / total if nobs_last is not None else np.nan\n",
    "\n",
    "    return reg_freq, sw_freq, swbs_freq, llr_mean, llr_sd, omega_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 0., 0.]), array([0.95, 0.03, 0.02]), array([0.96, 0.03, 0.01]), np.float64(10.923910277820518), np.float64(16.667382107985837), np.float64(18.116306391796517))\n"
     ]
    }
   ],
   "source": [
    "nobs = 250\n",
    "num_sims = 100\n",
    "c1 = .1\n",
    "setup_shi_ex  = lambda yn,xn: setup_shi(yn,xn)\n",
    "\n",
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=0)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.58, 0.  , 0.42]), array([0.95, 0.01, 0.04]), array([0.88, 0.  , 0.12]), np.float64(-21.003413238772723), np.float64(17.819538097378434), np.float64(21.15904454918971))\n"
     ]
    }
   ],
   "source": [
    "def gen_data3(beta= 1.5, nobs=1000):\n",
    "    cov = [[25, 0], [0, 1]]\n",
    "    data = np.random.multivariate_normal([0,beta], [[25,0],[0,1]],  nobs)\n",
    "    return data[:,0],data[:,1],nobs\n",
    "\n",
    "gen_data_ex = lambda : gen_data3(nobs=nobs,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.89, 0.  , 0.11]), array([0.99, 0.  , 0.01]), array([0.94, 0.  , 0.06]), np.float64(7.497537679255295), np.float64(35.2808516134166), np.float64(40.44404570305713))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.96, 0.  , 0.04]), array([0.95, 0.  , 0.05]), array([0.98, 0.  , 0.02]), np.float64(9.513749734943392), np.float64(108.47761766141838), np.float64(119.5181764266323))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.92, 0.01, 0.07]), array([0.96, 0.01, 0.03]), array([0.95, 0.01, 0.04]), np.float64(10.138419909366199), np.float64(58.24383431277212), np.float64(57.59413639150226))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=500,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# power 0.... need to see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 1.]), array([0.29, 0.  , 0.71]), array([0., 0., 1.]), np.float64(-269.6911084497565), np.float64(26.56245426883518), np.float64(32.95015887859984))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data3(nobs=nobs,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 1.]), array([0.04, 0.  , 0.96]), array([0., 0., 1.]), np.float64(-552.9026741818277), np.float64(32.7108627481665), np.float64(39.333260486239965))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data3(nobs=500,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,c1=c1)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
