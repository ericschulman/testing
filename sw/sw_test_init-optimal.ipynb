{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import vuong_tests10 as vuong_tests_fast\n",
    "from vuong_test_base import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "class JointNormal1(GenericLikelihoodModel):\n",
    "    \n",
    "    def loglikeobs(self, params):\n",
    "        data = np.concatenate([[self.endog],self.exog.transpose()],axis=0)\n",
    "        mult_rv = stats.multivariate_normal([params[0], 0.0], [[1,0],[0,1]])\n",
    "        return mult_rv.logpdf(data.transpose())\n",
    "    \n",
    "    \n",
    "class JointNormal2(GenericLikelihoodModel):\n",
    "    \n",
    "    def loglikeobs(self, params):\n",
    "        data = np.concatenate([[self.endog],self.exog.transpose()],axis=0)\n",
    "        mult_rv = stats.multivariate_normal([0.0, params[0]], [[1,0],[0,1]])\n",
    "        return mult_rv.logpdf(data.transpose())\n",
    "\n",
    "\n",
    "def setup_shi(yn,xn):\n",
    "    # model 1 grad, etc.\n",
    "    nobs = yn.shape[0]\n",
    "    model1_param = np.array([yn.mean()])\n",
    "    model2_param = np.array([xn.mean()])\n",
    "    \n",
    "    model1_deriv = JointNormal1(yn,xn)\n",
    "    ll1 = model1_deriv.loglikeobs(model1_param)\n",
    "    grad1 =  model1_deriv.score_obs(model1_param).reshape( (nobs,1) )\n",
    "    hess1 = model1_deriv.hessian(model1_param)\n",
    "    \n",
    "    \n",
    "    model2_deriv = JointNormal2(yn,xn)\n",
    "    ll2 = model2_deriv.loglikeobs(model2_param)\n",
    "    grad2 =  model2_deriv.score_obs(model2_param).reshape( (nobs,1) )  \n",
    "    hess2 = model2_deriv.hessian(model2_param)\n",
    "    \n",
    "    return ll1,grad1,hess1,model1_param,ll2,grad2,hess2,model2_param\n",
    "\n",
    "def gen_data(beta= 1.5, nobs=250):\n",
    "    cov = [[25, 0], [0, 1]]\n",
    "    data = np.random.multivariate_normal([beta,beta], [[25,0],[0,1]],  nobs)\n",
    "    return data[:,0],data[:,1],nobs\n",
    "\n",
    "\n",
    "def gen_data3(beta= 1.5, nobs=250):\n",
    "    cov = [[25, 0], [0, 1]]\n",
    "    data = np.random.multivariate_normal([0,beta], [[25,0],[0,1]],  nobs)\n",
    "    return data[:,0],data[:,1],nobs\n",
    "\n",
    "\n",
    "yn,xn,nobs = gen_data()\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "print(grad1.shape,hess1.shape)\n",
    "#NOTE! Weird size distortions with shi's test when theta = .5...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2796880101783003\n",
      "0\n",
      "0.26894212632500625\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def _estimate_HV(grad, hess_total, ridge=1e-8):\n",
    "    \"\"\"\n",
    "    grad: (n,p) per-observation score of the log-likelihood\n",
    "    hess_total: (p,p) full-sample Hessian of the log-likelihood (sum over i)\n",
    "    Returns:\n",
    "    H_hat = -E[∂² log f]  (per-observation average curvature)\n",
    "    V_hat = E[s s′]       (uncentered score second moment)\n",
    "    \"\"\"\n",
    "    n = grad.shape[0]\n",
    "    H_hat = -hess_total / n\n",
    "    S = grad - grad.mean() # i think its supposed to be centered....\n",
    "    V_hat = (S.T @ S) / n\n",
    "    return H_hat, V_hat\n",
    "\n",
    "def _trace_HinvV(H, V):\n",
    "    return float(np.trace(np.linalg.solve(H, V)))\n",
    "\n",
    "\n",
    "def diagnostics_grad_hess(grad, hess_total, name=\"\", ridge=1e-8):\n",
    "    n, p = grad.shape\n",
    "    # Per-observation H and V\n",
    "    H, V = estimate_HV_known_total(grad, hess_total, ridge=ridge)\n",
    "    \n",
    "    # Score mean (should be near 0 at MLE)\n",
    "    m = grad.mean(axis=0)\n",
    "    m_norm = float(np.linalg.norm(m))\n",
    "    # Scale of score variance\n",
    "    eigV = np.linalg.eigvalsh(V)\n",
    "    eigH = np.linalg.eigvalsh(H)\n",
    "    condH = np.linalg.cond(H)\n",
    "    \n",
    "    # Fisher-type check: V ≈ H if well-specified (rough check)\n",
    "    fisher_gap = float(np.linalg.norm(V - H, ord='fro'))  # not zero under misspecification\n",
    "    \n",
    "    trHinvV = trace_HinvV(H, V)\n",
    "    \n",
    "    print(f\"--- Diagnostics [{name}] ---\")\n",
    "    print(f\"n={n}, p={p}\")\n",
    "    print(f\"score mean norm ||E[s]|| ≈ {m_norm:.4g}  (should be small at MLE)\")\n",
    "    print(f\"H eig min/max = {eigH.min():.6g}/{eigH.max():.6g}, cond(H)={condH:.3g}\")\n",
    "    print(f\"V eig min/max = {eigV.min():.6g}/{eigV.max():.6g}\")\n",
    "    print(f\"tr(H^-1 V) = {trHinvV:.6g}  (≈ p if well-specified; O(p) in general)\")\n",
    "    print(f\"||V - H||_F = {fisher_gap:.6g}  (0 if well-specified; >0 under misspec.)\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "def compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, alpha=0.05, ridge=1e-8,\n",
    "                            min_epsilon=1e-6, max_epsilon=10.0):\n",
    "    nobs = ll1.shape[0]\n",
    "    idx_even = np.arange(0, nobs, 2)\n",
    "    idx_odd  = np.arange(1, nobs, 2)\n",
    "\n",
    "    # Variances\n",
    "    sigma2_hat  = max(float(np.var(ll1 - ll2, ddof=0)),1e-2) #NOTE cheating a little bit with epsilon... keep it big enough\n",
    "    sigmaA2_hat = float(np.var(ll1[idx_even], ddof=0)) if idx_even.size > 1 else 0.0\n",
    "    sigmaB2_hat = float(np.var(ll2[idx_odd],  ddof=0)) if idx_odd.size  > 1 else 0.0\n",
    "    sigma_hat   = np.sqrt(max(sigma2_hat, 1e-12))\n",
    "\n",
    "    # Estimate (H, V) for each model\n",
    "    H1_hat, V1_hat = _estimate_HV(grad1, hess1, ridge=ridge)\n",
    "    H2_hat, V2_hat = _estimate_HV(grad2, hess2, ridge=ridge)\n",
    "    try:\n",
    "        tr1 = abs(_trace_HinvV(H1_hat, V1_hat))\n",
    "        tr2 = abs(_trace_HinvV(H2_hat, V2_hat))\n",
    "        tr_max = max(tr1, tr2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        tr_max = 1.0\n",
    "\n",
    "    # Paper constants\n",
    "    z = norm.ppf(1 - alpha/2.0)\n",
    "    phi_z = norm.pdf(z)\n",
    "    delta_star_hat = (sigma_hat / 2.0) * (z - np.sqrt(4.0 + z**2))\n",
    "    phi_arg = z - (delta_star_hat / max(sigma_hat, 1e-12))\n",
    "    phi_term = norm.pdf(phi_arg)\n",
    "\n",
    "    denom_PL = 4.0 * (sigma_hat**3 + 1e-12)\n",
    "    CPL_num = delta_star_hat * (sigma_hat**2 - 2.0 * (sigmaA2_hat + sigmaB2_hat))\n",
    "    C_PL_hat = phi_term * CPL_num / denom_PL\n",
    "\n",
    "    denom_SD = np.sqrt(max((sigmaA2_hat + sigmaB2_hat)/2.0, 1e-12))\n",
    "    C_SD_hat = 2.0 * phi_z * tr_max / denom_SD\n",
    "\n",
    "    lnln_term = max(np.log(np.log(max(nobs, 3))), 1e-6)\n",
    "    fallback = nobs**(-1/6) * (lnln_term**(1/3))\n",
    "\n",
    "    valid = (np.isfinite(C_PL_hat) and np.isfinite(C_SD_hat) and\n",
    "             C_PL_hat > 0 and C_SD_hat > 0)\n",
    "\n",
    "    if valid:\n",
    "        eps_hat = ((C_SD_hat/C_PL_hat)**(1/3)) * (nobs**(-1/6)) * (lnln_term**(1/3))\n",
    "    else:\n",
    "        eps_hat = fallback\n",
    "\n",
    "    return float(np.clip(eps_hat, min_epsilon, max_epsilon))\n",
    "\n",
    "def sw_test_stat(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=.5,compute_v=True,print_stuff=False):\n",
    "    nobs = ll1.shape[0]\n",
    "\n",
    "    idx_even = np.arange(0, nobs, 2)         # 0-based indices: 0,2,4,...\n",
    "    idx_odd  = np.arange(1, nobs, 2)         # 1,3,5,...\n",
    "\n",
    "    # Regular loglikelihood ratio statistic\n",
    "    llr = (ll1 - ll2).sum()\n",
    "    # Split-sample difference statistic\n",
    "    llr_split = ll1[idx_even].sum() - ll2[idx_odd].sum()\n",
    "    # Regularized numerator (as in your expression)\n",
    "    llr_reg = llr + epsilon * llr_split\n",
    "    # Main variance\n",
    "    omega2 = (ll1 - ll2).var()\n",
    "    # Split-group variances\n",
    "    omega_A2 = ll1[idx_even].var()\n",
    "    omega_B2 = ll2[idx_odd].var()\n",
    "    # Regularized variance\n",
    "    omega_reg2 = (1 + epsilon) * omega2 + (epsilon**2 / 2) * (omega_A2 + omega_B2)\n",
    "    omega_reg = np.sqrt(omega_reg2)\n",
    "\n",
    "    if print_stuff:\n",
    "        print('llr',llr,llr_split)\n",
    "        print('omega',omega2,omega_A2,omega_B2,np.sqrt(omega2),np.sqrt(omega_reg2))\n",
    "    if not compute_v:\n",
    "        return llr_reg,omega_reg,nobs \n",
    "        \n",
    "    V = compute_eigen2(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2) # If you want to try to bias-correct you can, this is optional\n",
    "    return llr_reg,omega_reg,V,nobs \n",
    "    \n",
    "\n",
    "def sw_test(ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2,\n",
    "            epsilon=.5, alpha=0.05, biascorrect=False):\n",
    "\n",
    "        \n",
    "    llr_reg,omega_reg,V,nobs =  sw_test_stat(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=epsilon)\n",
    "    \n",
    "    if biascorrect:\n",
    "        llr_reg += V.sum() / 2\n",
    "\n",
    "    test_stat = llr_reg / (omega_reg * np.sqrt(nobs))\n",
    "\n",
    "    # Two-sided test\n",
    "    reject_high = (test_stat >= norm.ppf(1 - alpha / 2))\n",
    "    reject_low  = (test_stat <= norm.ppf(alpha / 2))\n",
    "    return int(reject_high) + 2 * int(reject_low)\n",
    "\n",
    "    \n",
    "yn,xn,nobs = gen_data()\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "\n",
    "\n",
    "reg_idx = regular_test(ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2)\n",
    "\n",
    "\n",
    "yn,xn,nobs = gen_data(nobs=250,beta=0)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "epsilon = compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, ridge=1e-8 , alpha=.05)\n",
    "print(epsilon)\n",
    "reg_idx = sw_test(ll1, grad1, hess1, params1,ll2, grad2, hess2, params2,epsilon =epsilon,   biascorrect=False, alpha=.05)\n",
    "print(reg_idx)\n",
    "\n",
    "yn,xn,nobs = gen_data3()\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_shi(yn,xn)\n",
    "epsilon = compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, ridge=1e-8 , alpha=.05)\n",
    "print(epsilon)\n",
    "reg_idx = sw_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2 ,epsilon =epsilon,   biascorrect=False, alpha=.05)\n",
    "print(reg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_distr(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "                    epsilon=0.5, trials=500, seed=None, biascorrect=False):\n",
    "    nobs = ll1.shape[0]\n",
    "    #print('---------')\n",
    "    llr_reg, omega_reg, V, nobs = sw_test_stat(\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=epsilon,print_stuff=False\n",
    "    )\n",
    "    if biascorrect:\n",
    "        llr_reg += V.sum() / 2\n",
    "    test_stat = llr_reg / (omega_reg * np.sqrt(nobs))\n",
    "    stat_dist = []\n",
    "\n",
    "    test_stat0 = (ll1-ll2).sum()/np.sqrt( nobs*(ll1-ll2).var() )\n",
    "    stats_s0 = []\n",
    "    \n",
    "    \n",
    "    for i in range(trials):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed + i)\n",
    "        sample = np.random.choice(np.arange(nobs), nobs, replace=True)\n",
    "        ll1_s = ll1[sample]\n",
    "        ll2_s = ll2[sample]\n",
    "\n",
    "        # Don't need V for bootstrap stats (saves computation)\n",
    "        llr_reg_s, omega_reg_s, nobs_in_s = sw_test_stat(\n",
    "            ll1_s, grad1, hess1, params1, ll2_s, grad2, hess2, params2, epsilon=epsilon, compute_v=False,print_stuff=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # bias correct V not directly available for bootstrap\n",
    "        stat_s = (llr_reg_s - llr_reg) / (omega_reg_s * np.sqrt(nobs_in_s))\n",
    "        stat_dist.append(stat_s)\n",
    "\n",
    "        ###### real stat for comparison...\n",
    "        llrs0 = ll1_s - ll2_s\n",
    "        stats_s0.append(  llrs0.sum()/  np.sqrt(nobs*llrs0.var()) )\n",
    "    if False:\n",
    "        print('test_stat0', nobs_in_s, nobs, (np.array(stats_s0)-test_stat0).mean(),np.array(stats_s0).mean(),test_stat0 )\n",
    "        print('test_stat', nobs_in_s, nobs, (np.array(stat_dist)-test_stat).mean(),np.array(stat_dist).mean(),test_stat )\n",
    "    #print('----')\n",
    "    return np.array(stat_dist), test_stat\n",
    "\n",
    "def sw_bs_test_helper(stat_dist, stat_obs, alpha=.05, left=True, right=True, print_stuff=False):\n",
    "    cv_upper = np.percentile(stat_dist, 100 * (1 - alpha / 2))\n",
    "    cv_lower = np.percentile(stat_dist, 100 * (alpha / 2))\n",
    "    if print_stuff:\n",
    "        print(f\"mean={stat_dist.mean():.3f}, cv_lower={cv_lower:.3f}, stat_obs={stat_obs:.3f}, cv_upper={cv_upper:.3f}\")\n",
    "    out = 0\n",
    "    if right and (stat_obs > cv_upper):\n",
    "        out = 1\n",
    "    if left and (stat_obs < cv_lower):\n",
    "        out += 2\n",
    "    return out\n",
    "\n",
    "def sw_bs_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "               alpha=.05, trials=500, epsilon=0.5, biascorrect=False, seed=None, print_stuff=False):\n",
    "    stat_dist, test_stat = bootstrap_distr(\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2,\n",
    "        epsilon=epsilon, trials=trials, seed=seed, biascorrect=biascorrect\n",
    "    )\n",
    "    return sw_bs_test_helper(stat_dist, test_stat, alpha=alpha, print_stuff=print_stuff)\n",
    "\n",
    "# Example Usage:\n",
    "# result = sw_bs_test(ll1, grad1, hess1, params1, ll2, grad2, hess2, params2, epsilon=0.5, trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo2(total, gen_data, setup_shi, alpha=.05,  epsilon=.5,trials=500, biascorrect=False,epsilon_auto=True):\n",
    "    # Count for [model1, tie, model2] for each test\n",
    "    reg = np.zeros(3, dtype=int)\n",
    "    sw  = np.zeros(3, dtype=int)\n",
    "    swbs = np.zeros(3, dtype=int)\n",
    "\n",
    "    # Tracking statistics\n",
    "    llr_sum = 0.0\n",
    "    llr_sq_sum = 0.0\n",
    "    omega_sum = 0.0\n",
    "    nobs_last = None\n",
    "\n",
    "    for a in range(total):\n",
    "        np.random.seed()\n",
    "        yn, xn, nobs = gen_data()\n",
    "        ll1, grad1, hess1, params1, ll2, grad2, hess2, params2 = setup_shi(yn, xn)\n",
    "        if epsilon_auto:\n",
    "            epsilon = compute_optimal_epsilon(ll1, grad1, hess1, params1,\n",
    "                 ll2, grad2, hess2, params2, ridge=1e-8 , alpha=alpha)\n",
    "        # Update LLR-related stats\n",
    "        llrn = (ll1 - ll2).sum()\n",
    "        omegan = np.sqrt((ll1 - ll2).var())\n",
    "        llr_sum   += llrn\n",
    "        llr_sq_sum += llrn ** 2\n",
    "        omega_sum += omegan\n",
    "        nobs_last = nobs\n",
    "\n",
    "        # Run the tests\n",
    "        reg_idx = regular_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2,\n",
    "            biascorrect=biascorrect, alpha=alpha\n",
    "        )\n",
    "        sw_idx = sw_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2, epsilon = epsilon,\n",
    "            alpha=alpha, biascorrect=biascorrect\n",
    "        )\n",
    "        swbs_idx = sw_bs_test(\n",
    "            ll1, grad1, hess1, params1,\n",
    "            ll2, grad2, hess2, params2, epsilon = epsilon,\n",
    "            alpha=alpha, biascorrect=biascorrect, print_stuff=False\n",
    "        )\n",
    "\n",
    "        reg[reg_idx]   += 1\n",
    "        sw[sw_idx]     += 1\n",
    "        swbs[swbs_idx] += 1\n",
    "\n",
    "    # Convert counts to rates/frequencies\n",
    "    reg_freq = reg / total\n",
    "    sw_freq = sw / total\n",
    "    swbs_freq = swbs / total\n",
    "\n",
    "    llr_mean = llr_sum / total\n",
    "    llr_sd   = np.sqrt(llr_sq_sum / total - llr_mean ** 2)\n",
    "    omega_scaled = omega_sum * np.sqrt(nobs_last) / total if nobs_last is not None else np.nan\n",
    "\n",
    "    return reg_freq, sw_freq, swbs_freq, llr_mean, llr_sd, omega_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 0., 0.]), array([0.95, 0.02, 0.03]), array([0.74, 0.15, 0.11]), np.float64(10.689338555657264), np.float64(13.677819674398188), np.float64(19.26042859809282))\n"
     ]
    }
   ],
   "source": [
    "nobs = 250\n",
    "num_sims = 100\n",
    "epsilon = .5\n",
    "setup_shi_ex  = lambda yn,xn: setup_shi(yn,xn)\n",
    "\n",
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=0)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon,epsilon_auto=False)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 0., 0.]), array([0.98, 0.02, 0.  ]), array([0.7 , 0.18, 0.12]), np.float64(11.143109705007605), np.float64(20.43285726640253), np.float64(17.65665739938718))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=0)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon,epsilon_auto=True)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.66, 0.  , 0.34]), array([0.94, 0.  , 0.06]), array([0.72, 0.07, 0.21]), np.float64(-18.903471101278846), np.float64(17.424700492168036), np.float64(22.050154635976128))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_data_ex = lambda : gen_data3(nobs=nobs,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.83, 0.  , 0.17]), array([0.96, 0.03, 0.01]), array([0.68, 0.2 , 0.12]), np.float64(6.191675890011836), np.float64(38.26157373552545), np.float64(39.81464107438891))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.88, 0.  , 0.12]), array([0.94, 0.02, 0.04]), array([0.72, 0.12, 0.16]), np.float64(5.886063884163076), np.float64(81.94521174705753), np.float64(79.53846857845777))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=1000,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.87, 0.02, 0.11]), array([0.94, 0.03, 0.03]), array([0.66, 0.12, 0.22]), np.float64(5.62725250623223), np.float64(149.1460921783497), np.float64(117.50634345822448))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=nobs,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.89, 0.  , 0.11]), array([0.94, 0.  , 0.06]), array([0.7 , 0.12, 0.18]), np.float64(15.075616167620483), np.float64(60.68412808480761), np.float64(59.19440294904288))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data(nobs=500,beta=.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# power 0.... need to see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 1.]), array([0.1, 0. , 0.9]), array([0.22, 0.  , 0.78]), np.float64(-267.7615912839913), np.float64(33.12834155614398), np.float64(34.28938522057534))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data3(nobs=nobs,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 1.]), array([0., 0., 1.]), array([0.04, 0.  , 0.96]), np.float64(-549.4799280151321), np.float64(39.172633841062556), np.float64(39.97946742179))\n"
     ]
    }
   ],
   "source": [
    "gen_data_ex = lambda : gen_data3(nobs=500,beta=1.5)\n",
    "mc_out = monte_carlo2(num_sims,gen_data_ex,setup_shi_ex,trials=500,epsilon=epsilon)\n",
    "print(mc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
